{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7cc67e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bibliotecas carregadas e configura√ß√µes iniciais prontas.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a URL base para completar os links relativos (/wiki/...)\n",
    "BASE_URL = \"https://hotwheels.fandom.com\"\n",
    "\n",
    "# Cria a pasta raiz para salvar os jsons brutos\n",
    "os.makedirs(\"json\", exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas carregadas e configura√ß√µes iniciais prontas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26647d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Lista definida: 5 link para processar.\n"
     ]
    }
   ],
   "source": [
    "# Lista de p√°ginas para varrer\n",
    "LIST_URL = [\n",
    "    \"https://hotwheels.fandom.com/wiki/List_of_1970_Hot_Wheels\",\n",
    "    \"https://hotwheels.fandom.com/wiki/List_of_1971_Hot_Wheels\",\n",
    "    \"https://hotwheels.fandom.com/wiki/List_of_1972_Hot_Wheels\",\n",
    "    \"https://hotwheels.fandom.com/wiki/List_of_1973_Hot_Wheels\",\n",
    "    # \"https://hotwheels.fandom.com/wiki/List_of_2026_Hot_Wheels\",\n",
    "    # \"https://hotwheels.fandom.com/wiki/List_of_2025_Hot_Wheels\",\n",
    "    # \"https://hotwheels.fandom.com/wiki/List_of_2024_Hot_Wheels\",\n",
    "    # Descomente as linhas acima para processar outros anos\n",
    "]\n",
    "\n",
    "print(f\"üìã Lista definida: {len(LIST_URL)} link para processar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5b1e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(el):\n",
    "    \"\"\"Limpa texto de elementos HTML removendo espa√ßos extras\"\"\"\n",
    "    if not el: return \"\"\n",
    "    return el.get_text(\" \", strip=True)\n",
    "\n",
    "def clean_key(text):\n",
    "    \"\"\"Transforma texto em slug limpo (ex: 'Mini Morris' -> 'mini-morris')\"\"\"\n",
    "    if not text: return \"unknown\"\n",
    "    # Decodifica caracteres de URL (ex: %20 vira espa√ßo)\n",
    "    import urllib.parse\n",
    "    text = urllib.parse.unquote(text)\n",
    "    # Remove caracteres especiais\n",
    "    text = re.sub(r'[^a-z0-9\\s-]', '', text.lower())\n",
    "    return re.sub(r'[\\s-]+', '-', text).strip('-')\n",
    "\n",
    "def parse_casting_page(url):\n",
    "    \"\"\"\n",
    "    Acessa a URL, l√™ a tabela e trata varia√ß√µes complexas e nomes ausentes.\n",
    "    \"\"\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. METADATA DO CASTING (L√ìGICA BLINDADA)\n",
    "    # ==========================================\n",
    "    infobox = soup.select_one(\"aside.portable-infobox\")\n",
    "    \n",
    "    casting_name = \"Unknown\"\n",
    "    debut_year = None\n",
    "    designer = \"Unknown\"\n",
    "    manufacturer = \"Unknown\"\n",
    "\n",
    "    # --- TENTATIVA 1: Infobox (H2) ---\n",
    "    if infobox:\n",
    "        h2 = infobox.select_one(\"h2\")\n",
    "        if h2: \n",
    "            casting_name = clean_text(h2)\n",
    "        \n",
    "        # Pega dados extras\n",
    "        for item in infobox.select(\".pi-item\"):\n",
    "            label_el = item.select_one(\".pi-data-label\")\n",
    "            value_el = item.select_one(\".pi-data-value\")\n",
    "            if label_el and value_el:\n",
    "                label = clean_text(label_el).lower()\n",
    "                value = clean_text(value_el)\n",
    "                if \"produced\" in label:\n",
    "                    match = re.search(r'\\d{4}', value)\n",
    "                    if match: debut_year = int(match.group(0))\n",
    "                if \"designer\" in label:\n",
    "                    designer = value\n",
    "\n",
    "    # --- TENTATIVA 2: Cabe√ßalho da P√°gina (H1) ---\n",
    "    if casting_name in [\"Unknown\", \"Unknown Model\", \"\"]:\n",
    "        header = soup.select_one(\"#firstHeading\") or soup.select_one(\"h1\")\n",
    "        if header:\n",
    "            casting_name = clean_text(header)\n",
    "\n",
    "    # --- TENTATIVA 3 (INFAL√çVEL): Nome via URL ---\n",
    "    # Se tudo falhar, pega o nome que est√° no link (ex: .../wiki/X-Steam)\n",
    "    if casting_name in [\"Unknown\", \"Unknown Model\", \"\"]:\n",
    "        # Pega a √∫ltima parte da URL e substitui _ por espa√ßo\n",
    "        url_name = url.split(\"/wiki/\")[-1]\n",
    "        import urllib.parse\n",
    "        casting_name = urllib.parse.unquote(url_name).replace(\"_\", \" \")\n",
    "\n",
    "    # Limpeza final do nome\n",
    "    casting_name = casting_name.replace(\" (Hot Wheels)\", \"\").strip()\n",
    "\n",
    "    # Define Fabricante\n",
    "    if casting_name and casting_name != \"Unknown\":\n",
    "        manufacturer = casting_name.split(\" \")[0]\n",
    "\n",
    "    casting_id = clean_key(casting_name)\n",
    "    \n",
    "    casting_obj = {\n",
    "        \"casting_id\": casting_id,\n",
    "        \"name\": casting_name,\n",
    "        \"description\": {\n",
    "            \"en-us\": \"\",\n",
    "            \"pt-br\": \"\"\n",
    "        },\n",
    "        \"designer\": designer,\n",
    "        \"debut_year\": debut_year,\n",
    "        \"manufacturer\": manufacturer,\n",
    "        \"releases\": [] \n",
    "    }\n",
    "\n",
    "    desc_p = soup.select(\"div.mw-parser-output > p\")\n",
    "    if desc_p:\n",
    "        casting_obj[\"description\"][\"en-us\"] = clean_text(desc_p[0])\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. PROCESSAMENTO DOS RELEASES\n",
    "    # ==========================================\n",
    "    \n",
    "    KNOWN_HEADERS = [\n",
    "        \"toy #\", \"toy id\", \"sku\", \"year\", \"series\", \"color\", \"body color\", \"cab color\", \n",
    "        \"tampo\", \"decoration\", \"base color / type\", \"base\", \"window color\", \"window\", \n",
    "        \"interior color\", \"interior\", \"wheel type\", \"wheels\", \"country\", \"notes\", \"photo\", \"image\"\n",
    "    ]\n",
    "\n",
    "    last_valid_values = {\n",
    "        \"year\": 0, \"series_raw\": \"\", \"series_id\": \"unknown\", \"series_index\": None,\n",
    "        \"color\": \"unknown\", \"country\": \"\", \"toy_number\": \"\" \n",
    "    }\n",
    "    \n",
    "    last_extra_values = {}\n",
    "    generated_ids_count = {} \n",
    "    current_release = None \n",
    "\n",
    "    for table in soup.select(\"table.wikitable\"):\n",
    "        headers = [clean_text(th) for th in table.select(\"th\")]\n",
    "        headers_map = {h.lower(): i for i, h in enumerate(headers)}\n",
    "        \n",
    "        if \"toy #\" not in headers_map and \"col #\" not in headers_map: continue\n",
    "\n",
    "        for tr in table.select(\"tr\")[1:]:\n",
    "            tds = tr.select(\"td\")\n",
    "            if not tds: continue\n",
    "\n",
    "            def get_val(key_list):\n",
    "                for k in key_list:\n",
    "                    idx = headers_map.get(k.lower())\n",
    "                    if idx is not None and idx < len(tds):\n",
    "                        return clean_text(tds[idx])\n",
    "                return \"\"\n",
    "\n",
    "            # --- Extra√ß√£o ---\n",
    "            toy_val_cell = get_val([\"toy #\", \"toy id\", \"sku\"])\n",
    "            if toy_val_cell: last_valid_values[\"toy_number\"] = toy_val_cell\n",
    "\n",
    "            year_str = get_val([\"year\"])\n",
    "            if year_str and year_str.isdigit(): last_valid_values[\"year\"] = int(year_str)\n",
    "            \n",
    "            series_raw = get_val([\"series\"])\n",
    "            if series_raw:\n",
    "                last_valid_values[\"series_raw\"] = series_raw\n",
    "                last_valid_values[\"series_id\"] = clean_key(series_raw.split(\" \")[0])\n",
    "                last_valid_values[\"series_index\"] = None\n",
    "                if \"/\" in series_raw:\n",
    "                    try:\n",
    "                        match = re.search(r'(\\d+)/\\d+', series_raw)\n",
    "                        if match: last_valid_values[\"series_index\"] = int(match.group(1))\n",
    "                    except: pass\n",
    "\n",
    "            color_val = get_val([\"color\", \"body color\", \"cab color\"])\n",
    "            if color_val: last_valid_values[\"color\"] = color_val\n",
    "\n",
    "            country_val = get_val([\"country\"])\n",
    "            if country_val: last_valid_values[\"country\"] = country_val\n",
    "\n",
    "            tampo = get_val([\"tampo\", \"decoration\"])\n",
    "            notes = get_val([\"notes\"])\n",
    "            interior = get_val([\"interior color\", \"interior\"])\n",
    "            \n",
    "            base_raw = get_val([\"base color / type\", \"base\"])\n",
    "            base_color = \"\"\n",
    "            base_type = \"\"\n",
    "            if \"/\" in base_raw:\n",
    "                parts = base_raw.split(\"/\")\n",
    "                base_color = parts[0].strip()\n",
    "                base_type = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "            else:\n",
    "                base_type = base_raw \n",
    "\n",
    "            wheel_val = get_val([\"wheel type\", \"wheels\"])\n",
    "            \n",
    "            img_url = \"\"\n",
    "            img_tag = tr.select_one(\"img\")\n",
    "            if img_tag:\n",
    "                src = img_tag.get(\"data-src\") or img_tag.get(\"src\")\n",
    "                if src: img_url = re.sub(r'/scale-to-width-down/\\d+', '', src)\n",
    "\n",
    "            # Extras Din√¢micos\n",
    "            current_extras = {}\n",
    "            for header_txt, idx in headers_map.items():\n",
    "                if header_txt not in KNOWN_HEADERS:\n",
    "                    val = clean_text(tds[idx]) if idx < len(tds) else \"\"\n",
    "                    slug_key = clean_key(header_txt).replace(\"-\", \"_\")\n",
    "                    if val:\n",
    "                        last_extra_values[slug_key] = val\n",
    "                        current_extras[slug_key] = val\n",
    "                    else:\n",
    "                        current_extras[slug_key] = last_extra_values.get(slug_key, \"\")\n",
    "\n",
    "            # --- Decis√£o ---\n",
    "            is_new_entry = False\n",
    "            if toy_val_cell: is_new_entry = True\n",
    "            elif last_valid_values[\"toy_number\"] and (tampo or interior or base_raw): is_new_entry = True\n",
    "\n",
    "            if is_new_entry:\n",
    "                final_toy = toy_val_cell if toy_val_cell else last_valid_values[\"toy_number\"]\n",
    "                final_year = int(year_str) if year_str.isdigit() else last_valid_values[\"year\"]\n",
    "                final_series_id = last_valid_values[\"series_id\"]\n",
    "                final_series_index = last_valid_values[\"series_index\"]\n",
    "                final_color = color_val if color_val else last_valid_values[\"color\"]\n",
    "                final_country = country_val if country_val else last_valid_values[\"country\"]\n",
    "\n",
    "                unique_suffix = clean_key(final_color)\n",
    "                toy_slug = clean_key(final_toy)\n",
    "                base_id = f\"{final_year}-{casting_id}-{unique_suffix}-{toy_slug}\"\n",
    "                \n",
    "                if base_id in generated_ids_count:\n",
    "                    generated_ids_count[base_id] += 1\n",
    "                    release_id = f\"{base_id}-v{generated_ids_count[base_id]}\"\n",
    "                else:\n",
    "                    generated_ids_count[base_id] = 1\n",
    "                    release_id = base_id\n",
    "\n",
    "                specs_final = {\n",
    "                    \"color\": final_color, \"tampo\": tampo, \"base_color\": base_color, \"base_type\": base_type,\n",
    "                    \"window_color\": get_val([\"window color\", \"window\"]), \"interior_color\": interior,\n",
    "                    \"wheel_type\": {\"0\": wheel_val}\n",
    "                }\n",
    "                specs_final.update(current_extras)\n",
    "\n",
    "                release_data = {\n",
    "                    \"release_id\": release_id, \"toy_number\": final_toy, \"casting_id\": casting_id,\n",
    "                    \"year\": final_year, \"series_id\": final_series_id, \"series_index\": final_series_index,\n",
    "                    \"specs\": specs_final, \"country\": final_country, \"notes\": notes, \"images\": {\"0\": img_url}\n",
    "                }\n",
    "                \n",
    "                casting_obj[\"releases\"].append(release_data)\n",
    "                current_release = release_data \n",
    "            \n",
    "            elif current_release:\n",
    "                if wheel_val:\n",
    "                    existing_wheels = list(current_release[\"specs\"][\"wheel_type\"].values())\n",
    "                    if wheel_val not in existing_wheels:\n",
    "                        idx = str(len(current_release[\"specs\"][\"wheel_type\"]))\n",
    "                        current_release[\"specs\"][\"wheel_type\"][idx] = wheel_val\n",
    "                if img_url:\n",
    "                     existing_imgs = list(current_release[\"images\"].values())\n",
    "                     if img_url not in existing_imgs:\n",
    "                        idx = str(len(current_release[\"images\"]))\n",
    "                        current_release[\"images\"][idx] = img_url\n",
    "\n",
    "    return [casting_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a5e6f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Processando lista: https://hotwheels.fandom.com/wiki/List_of_1969_Hot_Wheels\n",
      "   üìÇ 24 links encontrados. Salvando em: json/batch_1969/\n",
      "\n",
      "‚è≥ Processando 4.17% (1/24): 'Classic '32 Ford Vicky' (classic-32-ford-vicky)...\n",
      "‚è≥ Processando 8.33% (2/24): 'Classic '31 Ford Woody' (classic-31-ford-woody)...\n",
      "‚è≥ Processando 12.5% (3/24): 'Classic '57 T-Bird' (classic-57-t-bird)...\n",
      "‚è≥ Processando 16.67% (4/24): 'Classic '36 Ford Coupe' (classic-36-ford-coupe)...\n",
      "‚è≥ Processando 20.83% (5/24): 'Twin Mill' (twin-mill)...\n",
      "‚è≥ Processando 25% (6/24): 'Turbofire' (turbofire)...\n",
      "‚è≥ Processando 29.17% (7/24): 'Torero' (torero)...\n",
      "‚è≥ Processando 33.33% (8/24): 'Splittin' Image' (splittin-image)...\n",
      "‚è≥ Processando 37.5% (9/24): 'Custom Continental Mark III' (custom-continental-mark-iii)...\n",
      "‚è≥ Processando 41.67% (10/24): 'Custom AMX' (custom-amx)...\n",
      "‚è≥ Processando 45.83% (11/24): 'Custom Charger' (custom-charger)...\n",
      "‚è≥ Processando 50% (12/24): 'Custom Police Cruiser' (custom-police-cruiser)...\n",
      "‚è≥ Processando 54.17% (13/24): 'Volkswagen Beach Bomb' (volkswagen-beach-bomb)...\n",
      "‚è≥ Processando 58.33% (14/24): 'Mercedes-Benz 280SL' (mercedes-benz-280sl)...\n",
      "‚è≥ Processando 62.5% (15/24): 'Rolls-Royce Silver Shadow' (rolls-royce-silver-shadow)...\n",
      "‚è≥ Processando 66.67% (16/24): 'Maserati Mistral' (maserati-mistral)...\n",
      "‚è≥ Processando 70.83% (17/24): 'Lola GT70' (lola-gt70)...\n",
      "‚è≥ Processando 75% (18/24): 'McLaren M6A' (mclaren-m6a)...\n",
      "‚è≥ Processando 79.17% (19/24): 'Chaparral 2G' (chaparral-2g)...\n",
      "‚è≥ Processando 83.33% (20/24): 'Ford Mark IV' (ford-mark-iv)...\n",
      "‚è≥ Processando 87.5% (21/24): 'Lotus Turbine' (lotus-turbine)...\n",
      "‚è≥ Processando 91.67% (22/24): 'Indy Eagle' (indy-eagle)...\n",
      "‚è≥ Processando 95.83% (23/24): 'Brabham Repco F1' (brabham-repco-f1)...\n",
      "‚è≥ Processando 100% (24/24): 'Shelby Turbine' (shelby-turbine)...\n",
      "üìÑ Processando lista: https://hotwheels.fandom.com/wiki/List_of_1970_Hot_Wheels\n",
      "   üìÇ 0 links encontrados. Salvando em: json/batch_1970/\n",
      "\n",
      "üìÑ Processando lista: https://hotwheels.fandom.com/wiki/List_of_1971_Hot_Wheels\n",
      "   üìÇ 0 links encontrados. Salvando em: json/batch_1971/\n",
      "\n",
      "üìÑ Processando lista: https://hotwheels.fandom.com/wiki/List_of_1972_Hot_Wheels\n",
      "   üìÇ 0 links encontrados. Salvando em: json/batch_1972/\n",
      "\n",
      "üìÑ Processando lista: https://hotwheels.fandom.com/wiki/List_of_1973_Hot_Wheels\n",
      "   üìÇ 0 links encontrados. Salvando em: json/batch_1973/\n",
      "\n",
      "\n",
      "‚úÖ Processamento do Scraping conclu√≠do com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Conjunto global para rastrear castings j√° processados e evitar re-download nesta sess√£o\n",
    "processed_castings_ids = set()\n",
    "\n",
    "for list_url in LIST_URL:\n",
    "    print(f\"üìÑ Processando lista: {list_url}\")\n",
    "\n",
    "    resp = requests.get(list_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Define nome da pasta do lote (ex: json/batch_2026)\n",
    "    page_name = list_url.split(\"/wiki/List_of_\")[-1].split(\"_Hot_Wheels\")[0]\n",
    "    batch_name = f\"batch_{page_name}\"\n",
    "    output_dir = f\"json/{batch_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    links = {}\n",
    "\n",
    "    # Encontra links na tabela principal da Wiki\n",
    "    for table in soup.select(\"table.wikitable\"):\n",
    "        # Geralmente o link do carro est√° na coluna\n",
    "        for col_idx in [2, 3]:\n",
    "            for a in table.select(f\"td:nth-child({col_idx}) a\"):\n",
    "                name = clean_text(a)\n",
    "                # Ignora links de \"2nd Color\" pois levam para a mesma p√°gina\n",
    "                if \"2nd Color\" in name:\n",
    "                    continue\n",
    "                href = a.get(\"href\", \"\")\n",
    "                if href.startswith(\"/wiki/\"):\n",
    "                    links[name] = BASE_URL + href\n",
    "\n",
    "    total_links = len(links)\n",
    "    print(f\"   üìÇ {total_links} links encontrados. Salvando em: {output_dir}/\\n\")\n",
    "\n",
    "    # Loop para entrar em cada carro\n",
    "    for i, (name, url) in enumerate(links.items(), start=1):\n",
    "        casting_id = clean_key(name)\n",
    "        \n",
    "        # Opcional: Se quiser pular carros j√° processados\n",
    "        if casting_id in processed_castings_ids:\n",
    "             # print(f\"   ‚è© Pulando '{name}' - j√° processado.\")\n",
    "             # continue \n",
    "             pass\n",
    "            \n",
    "        # C√°lculo da porcentagem visual (remove zeros extras)\n",
    "        pct = str(round((i/total_links)*100, 2)).rstrip('0').rstrip('.')\n",
    "        \n",
    "        print(f\"‚è≥ Processando {pct}% ({i}/{total_links}): '{name}' ({casting_id})...\")\n",
    "        \n",
    "        try:\n",
    "            # Chama a fun√ß√£o principal (C√©lula 3)\n",
    "            data = parse_casting_page(url)\n",
    "            \n",
    "            # O retorno √© uma lista, mas geralmente s√≥ tem 1 item (o carro)\n",
    "            if data and len(data) > 0:\n",
    "                final_obj = data[0]\n",
    "                # Se o nome veio como \"Unknown\" mesmo ap√≥s as tentativas, usamos o nome do link\n",
    "                if final_obj[\"name\"] == \"Unknown\":\n",
    "                    final_obj[\"name\"] = name\n",
    "                    final_obj[\"casting_id\"] = casting_id\n",
    "\n",
    "                # Usa o ID real obtido no scraping para salvar o arquivo\n",
    "                filename = final_obj[\"casting_id\"]\n",
    "                \n",
    "                with open(f\"{output_dir}/{filename}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "                \n",
    "                processed_castings_ids.add(filename)\n",
    "            \n",
    "            # Delay √©tico para n√£o sobrecarregar a Wiki\n",
    "            time.sleep(1) \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar ({i}/{total_links}) {name}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Processamento do Scraping conclu√≠do com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb157da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto global para rastrear castings j√° processados\n",
    "processed_castings_ids = set()\n",
    "\n",
    "# Headers de tabela para anos normais\n",
    "NAME_HEADERS = [\"model\", \"name\", \"casting\", \"car\", \"model name\"]\n",
    "\n",
    "for list_url in LIST_URL:\n",
    "    print(f\"üìÑ Processando lista: {list_url}\")\n",
    "\n",
    "    resp = requests.get(list_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    page_name = list_url.split(\"/wiki/List_of_\")[-1].split(\"_Hot_Wheels\")[0]\n",
    "    batch_name = f\"batch_{page_name}\"\n",
    "    output_dir = f\"json/{batch_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    links = {}\n",
    "\n",
    "    # --- ESTRAT√âGIA 1: Tabelas (Padr√£o) ---\n",
    "    for table in soup.select(\"table.wikitable\"):\n",
    "        # Descobre qual coluna tem o nome\n",
    "        headers = [th.get_text(\" \", strip=True).lower() for th in table.select(\"tr:first-child th, thead tr th\")]\n",
    "        target_col_idxs = []\n",
    "        for i, h in enumerate(headers):\n",
    "            if any(x in h for x in NAME_HEADERS):\n",
    "                target_col_idxs.append(i)\n",
    "        if not target_col_idxs: target_col_idxs = [1, 2] # Fallback\n",
    "\n",
    "        for tr in table.select(\"tr\")[1:]:\n",
    "            tds = tr.select(\"td\")\n",
    "            for idx in target_col_idxs:\n",
    "                if idx < len(tds):\n",
    "                    a = tds[idx].select_one(\"a\")\n",
    "                    if a:\n",
    "                        name = clean_text(a)\n",
    "                        href = a.get(\"href\", \"\")\n",
    "                        if \"2nd Color\" in name or \"File:\" in href: continue\n",
    "                        if href.startswith(\"/wiki/\") and \"List_of\" not in href:\n",
    "                            if name not in links: links[name] = BASE_URL + href\n",
    "\n",
    "    # --- ESTRAT√âGIA 2: Galerias (Anos 1970-1973) ---\n",
    "    # A Wiki transforma <gallery> em divs com a classe 'gallerytext'\n",
    "    # Dentro deles tem o link <a> com o nome do carro\n",
    "    for gallery_item in soup.select(\"div.gallerytext\"):\n",
    "        a = gallery_item.select_one(\"a\")\n",
    "        if a:\n",
    "            name = clean_text(a)\n",
    "            href = a.get(\"href\", \"\")\n",
    "            \n",
    "            # Valida√ß√µes\n",
    "            if not name: continue\n",
    "            if \"File:\" in href: continue # Ignora link para o arquivo de imagem\n",
    "            if \"List_of\" in href: continue # Ignora links para outras listas\n",
    "            \n",
    "            if href.startswith(\"/wiki/\"):\n",
    "                if name not in links:\n",
    "                    links[name] = BASE_URL + href\n",
    "\n",
    "    total_links = len(links)\n",
    "    print(f\"   üìÇ {total_links} links encontrados. Salvando em: {output_dir}/\\n\")\n",
    "\n",
    "    # Loop de Extra√ß√£o (Mantido igual)\n",
    "    for i, (name, url) in enumerate(links.items(), start=1):\n",
    "        casting_id = clean_key(name)\n",
    "        \n",
    "        if casting_id in processed_castings_ids: pass\n",
    "            \n",
    "        pct = str(round((i/total_links)*100, 2)).rstrip('0').rstrip('.')\n",
    "        print(f\"‚è≥ Processando {pct}% ({i}/{total_links}): '{name}' ({casting_id})...\")\n",
    "        \n",
    "        try:\n",
    "            data = parse_casting_page(url)\n",
    "            \n",
    "            if data and len(data) > 0:\n",
    "                final_obj = data[0]\n",
    "                if final_obj[\"name\"] in [\"Unknown\", \"Unknown Model\"]:\n",
    "                    final_obj[\"name\"] = name\n",
    "                    final_obj[\"casting_id\"] = casting_id\n",
    "\n",
    "                filename = final_obj[\"casting_id\"]\n",
    "                with open(f\"{output_dir}/{filename}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "                \n",
    "                processed_castings_ids.add(filename)\n",
    "            \n",
    "            time.sleep(1) \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar ({i}/{total_links}) {name}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Processamento do Scraping conclu√≠do com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
